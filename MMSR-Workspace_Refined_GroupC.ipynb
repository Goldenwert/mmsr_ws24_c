{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b252d9d3-7f3f-4ad7-a9d9-866a379c1278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import ast\n",
    "import os\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "DATA_DIR = 'data/'\n",
    "\n",
    "#all file paths\n",
    "INFORMATION_FILE = os.path.join(DATA_DIR, 'id_information_mmsr.tsv')\n",
    "GENRES_FILE = os.path.join(DATA_DIR, 'id_genres_mmsr.tsv')\n",
    "LYRICS_TFIDF_FILE = os.path.join(DATA_DIR, 'id_lyrics_tf-idf_mmsr.tsv')\n",
    "LYRICS_BERT_FILE = os.path.join(DATA_DIR, 'id_lyrics_bert_mmsr.tsv')\n",
    "MFCC_BOW_FILE = os.path.join(DATA_DIR, 'id_mfcc_bow_mmsr.tsv')\n",
    "SPECTRAL_CONTRAST_FILE = os.path.join(DATA_DIR, 'id_blf_spectralcontrast_mmsr.tsv')\n",
    "VGG19_FILE = os.path.join(DATA_DIR, 'id_vgg19_mmsr.tsv')\n",
    "RESNET_FILE = os.path.join(DATA_DIR, 'id_resnet_mmsr.tsv')\n",
    "TAGS_FILE = os.path.join(DATA_DIR, 'id_tags_dict.tsv')\n",
    "FILTERED_TAGS_FILE = os.path.join(DATA_DIR, 'filtered_id_tags_dict.tsv')\n",
    "METADATA_FILE = os.path.join(DATA_DIR, 'id_metadata_mmsr.tsv')\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def load_dataframe(file_path, sep='\\t', header='infer', names=None):\n",
    "    \"\"\"\n",
    "    Utility function to load a TSV file into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=sep, header=header, names=names)\n",
    "        print(f\"Loaded DataFrame from '{file_path}' with shape: {df.shape}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "        exit(1)\n",
    "\n",
    "#load datasets\n",
    "information_df = load_dataframe(INFORMATION_FILE)\n",
    "genres_df = load_dataframe(GENRES_FILE)\n",
    "metadata_df = load_dataframe(METADATA_FILE)\n",
    "tags_df = load_dataframe(TAGS_FILE, header=None, names=['id', 'tags_str'])\n",
    "\n",
    "lyrics_tfidf_df = load_dataframe(LYRICS_TFIDF_FILE)\n",
    "tfidf_cols = [col for col in lyrics_tfidf_df.columns if col != 'id']\n",
    "lyrics_tfidf_df.rename(columns={col: f\"tfidf_{col}\" for col in tfidf_cols}, inplace=True)\n",
    "\n",
    "bert_df = load_dataframe(LYRICS_BERT_FILE)\n",
    "bert_feature_columns = [col for col in bert_df.columns if col != 'id']\n",
    "bert_df.rename(columns={col: f\"bert_{col}\" for col in bert_feature_columns}, inplace=True)\n",
    "\n",
    "mfcc_bow_df = load_dataframe(MFCC_BOW_FILE)\n",
    "mfcc_bow_columns = [col for col in mfcc_bow_df.columns if col != 'id']\n",
    "mfcc_bow_df.rename(columns={col: f\"mfcc_{col}\" for col in mfcc_bow_columns}, inplace=True)\n",
    "\n",
    "spectral_contrast_df = load_dataframe(SPECTRAL_CONTRAST_FILE)\n",
    "spectral_contrast_columns = [col for col in spectral_contrast_df.columns if col != 'id']\n",
    "spectral_contrast_df.rename(columns={col: f\"spectral_{col}\" for col in spectral_contrast_columns}, inplace=True)\n",
    "\n",
    "vgg19_df = load_dataframe(VGG19_FILE)\n",
    "vgg19_feature_columns = [col for col in vgg19_df.columns if col != 'id']\n",
    "vgg19_df.rename(columns={col: f\"vgg19_{col}\" for col in vgg19_feature_columns}, inplace=True)\n",
    "\n",
    "resnet_df = load_dataframe(RESNET_FILE)\n",
    "resnet_feature_columns = [col for col in resnet_df.columns if col != 'id']\n",
    "resnet_df.rename(columns={col: f\"resnet_{col}\" for col in resnet_feature_columns}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# dataframes manipulations\n",
    "\n",
    "\n",
    "catalog_df = pd.merge(information_df, metadata_df[['id', 'popularity']], on='id', how='left')\n",
    "\n",
    "def parse_genres(genre_str):\n",
    "    if pd.isnull(genre_str):\n",
    "        return []\n",
    "    return [genre.strip().lower() for genre in genre_str.split(',')]\n",
    "\n",
    "genres_df['genre'] = genres_df['genre'].apply(parse_genres)\n",
    "\n",
    "#update catalog_df\n",
    "catalog_df = pd.merge(catalog_df, genres_df, on='id', how='left')\n",
    "\n",
    "#assigning an empty list\n",
    "catalog_df['genre'] = catalog_df['genre'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "def get_top_genre(genres_list):\n",
    "    if not genres_list:\n",
    "        return None\n",
    "    return genres_list[0]\n",
    "\n",
    "#determine the top genre\n",
    "catalog_df['top_genre'] = catalog_df['genre'].apply(get_top_genre)\n",
    "\n",
    "\n",
    "\n",
    "#5. tags with weights\n",
    "\n",
    "catalog_df = pd.merge(catalog_df, tags_df[['id', 'tags_str']], on='id', how='left')\n",
    "\n",
    "#assigning an empty string\n",
    "catalog_df['tags_str'] = catalog_df['tags_str'].fillna('{}')\n",
    "\n",
    "def parse_tags_and_weights(tag_weight_str):\n",
    "    \"\"\"\n",
    "    Parses the 'tags_str' string into separate lists of tags and weights.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #string to a dictionary\n",
    "        tag_weight_dict = ast.literal_eval(tag_weight_str)\n",
    "        if isinstance(tag_weight_dict, dict):\n",
    "            tags = list(tag_weight_dict.keys())\n",
    "            weights = list(tag_weight_dict.values())\n",
    "            return tags, weights\n",
    "        else:\n",
    "            print(f\"Warning: Expected dict, got {type(tag_weight_dict)}\")\n",
    "            return [], []\n",
    "    except (ValueError, SyntaxError) as e:\n",
    "        print(f\"Error parsing tags: {e}\")\n",
    "        return [], []\n",
    "\n",
    "#create 'tags' and 'weights' columns\n",
    "catalog_df[['tags', 'weights']] = catalog_df.apply(\n",
    "    lambda row: pd.Series(parse_tags_and_weights(row['tags_str'])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess_tag(tag):\n",
    "    \"\"\"\n",
    "    Preprocesses a single tag: lowercases, removes punctuation, and lemmatizes.\n",
    "    \"\"\"\n",
    "    tag = tag.lower()\n",
    "    tag = tag.translate(str.maketrans('', '', string.punctuation))\n",
    "    tag = lemmatizer.lemmatize(tag)\n",
    "    return tag\n",
    "\n",
    "# Apply preprocessing to all tags\n",
    "catalog_df['processed_tags'] = catalog_df.apply(\n",
    "    lambda row: [preprocess_tag(tag) for tag in row['tags']],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "#exclude genres\n",
    "\n",
    "genre_tags = set()\n",
    "for genres in catalog_df['genre']:\n",
    "    for genre in genres:\n",
    "        genre_tags.add(preprocess_tag(genre))\n",
    "\n",
    "#add 'alternative' and 'indie' to genre_tags\n",
    "genre_tags.update(['alternative', 'indie'])\n",
    "\n",
    "print(f\"\\nGenre Tags to Exclude: {genre_tags}\")\n",
    "\n",
    "def exclude_genre_tags(tags, genre_tags):\n",
    "    \"\"\"\n",
    "    Excludes any tag that matches any genre tag exactly.\n",
    "    \"\"\"\n",
    "    return [tag for tag in tags if tag not in genre_tags]\n",
    "\n",
    "#exclusion of genre tags\n",
    "catalog_df['filtered_processed_tags'] = catalog_df.apply(\n",
    "    lambda row: exclude_genre_tags(row['processed_tags'], genre_tags),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "#filter tags using thresholds\n",
    "\n",
    "#weight threshold\n",
    "min_weight_threshold = 50\n",
    "\n",
    "#column 'filtered_processed_tags_final' retains only tags with weight >= threshold\n",
    "catalog_df['filtered_processed_tags_final'] = catalog_df.apply(\n",
    "    lambda row: [tag for tag, weight in zip(row['filtered_processed_tags'], row['weights']) if weight >= min_weight_threshold],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "#filter weights\n",
    "catalog_df['filtered_weights_final'] = catalog_df.apply(\n",
    "    lambda row: [weight for tag, weight in zip(row['filtered_processed_tags'], row['weights']) if weight >= min_weight_threshold],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "#sample of the catalog_df to verify the filtering\n",
    "sample_tracks = catalog_df[['id', 'artist', 'song', 'filtered_processed_tags_final', 'filtered_weights_final']].sample(5, random_state=42)\n",
    "\n",
    "\n",
    "num_no_tags = catalog_df[catalog_df['filtered_processed_tags_final'].apply(len) == 0].shape[0]\n",
    "total_tracks = catalog_df.shape[0]\n",
    "percentage_no_tags = (num_no_tags / total_tracks) * 100\n",
    "catalog_df_filtered = catalog_df[catalog_df['filtered_processed_tags_final'].apply(len) > 0].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#vectorization with TF-IDF\n",
    "\n",
    "\n",
    "def vectorize_tags_tfidf(catalog_df, min_df=1):\n",
    "    \"\"\"\n",
    "    Vectorizes tags using TF-IDF.\n",
    "    \n",
    "    Parameters:\n",
    "    - catalog_df (pd.DataFrame): DataFrame containing 'filtered_processed_tags_final'.\n",
    "    - min_df (int): Minimum document frequency for a tag to be included.\n",
    "    \n",
    "    Returns:\n",
    "    - tag_matrix_tfidf (csr_matrix): TF-IDF normalized tag matrix.\n",
    "    - vectorizer (TfidfVectorizer): Fitted TF-IDF vectorizer.\n",
    "    \"\"\"\n",
    "    #filtered_processed_tags_final into strings\n",
    "    catalog_df['tags_str_final_tfidf'] = catalog_df['filtered_processed_tags_final'].apply(lambda tags: ' '.join(tags))\n",
    "    \n",
    "    #TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer(min_df=min_df)\n",
    "    \n",
    "    tag_matrix_tfidf = vectorizer.fit_transform(catalog_df['tags_str_final_tfidf'])\n",
    "    print(f\"TF-IDF Tag Matrix Shape: {tag_matrix_tfidf.shape}\")\n",
    "    \n",
    "    return tag_matrix_tfidf, vectorizer\n",
    "\n",
    "#TF-IDF Vectorization on Filtered Catalog\n",
    "tag_matrix_tfidf, tfidf_vectorizer = vectorize_tags_tfidf(catalog_df_filtered, min_df=1)\n",
    "\n",
    "\n",
    "\n",
    "#vectorization with Binary Encoding for Tag-Based Retrieval\n",
    "\n",
    "def vectorize_tags_binary(catalog_df, min_df=1):\n",
    "    \"\"\"\n",
    "    Vectorizes tags using binary encoding (presence/absence).\n",
    "    \n",
    "    Parameters:\n",
    "    - catalog_df (pd.DataFrame): DataFrame containing 'filtered_processed_tags_final'.\n",
    "    - min_df (int): Minimum document frequency for a tag to be included.\n",
    "    \n",
    "    Returns:\n",
    "    - tag_matrix_binary (csr_matrix): Binary tag matrix.\n",
    "    - vectorizer (CountVectorizer): Fitted CountVectorizer with binary encoding.\n",
    "    \"\"\"\n",
    "    #filtered_processed_tags_final into strings\n",
    "    catalog_df['tags_str_final_binary'] = catalog_df['filtered_processed_tags_final'].apply(lambda tags: ' '.join(tags))\n",
    "    \n",
    "    #CountVectorizer with binary=True\n",
    "    vectorizer = CountVectorizer(binary=True, min_df=min_df)\n",
    "    \n",
    "    tag_matrix_binary = vectorizer.fit_transform(catalog_df['tags_str_final_binary'])\n",
    "    print(f\"Binary Tag Matrix Shape: {tag_matrix_binary.shape}\")\n",
    "    \n",
    "    return tag_matrix_binary, vectorizer\n",
    "\n",
    "#Binary Vectorization on Filtered Catalog\n",
    "tag_matrix_binary, binary_vectorizer = vectorize_tags_binary(catalog_df_filtered, min_df=1)\n",
    "\n",
    "\n",
    "\n",
    "#Feature Matrices for Retrieval Systems\n",
    "\n",
    "\n",
    "#merge features and ensure alignment\n",
    "def merge_features(catalog_df_filtered, feature_df, prefix):\n",
    "    \"\"\"\n",
    "    Merges a feature DataFrame with catalog_df_filtered on 'id'.\n",
    "    Renames feature columns with the given prefix.\n",
    "    \"\"\"\n",
    "    feature_df = pd.merge(catalog_df_filtered[['id']], feature_df, on='id', how='left')\n",
    "    feature_cols = [col for col in feature_df.columns if col != 'id']\n",
    "    feature_df.rename(columns={col: f\"{prefix}_{col}\" for col in feature_cols}, inplace=True)\n",
    "    return feature_df\n",
    "\n",
    "#merge and align all feature matrices\n",
    "merged_bert_df = merge_features(catalog_df_filtered, bert_df, 'bert')\n",
    "merged_mfcc_bow_df = merge_features(catalog_df_filtered, mfcc_bow_df, 'mfcc')\n",
    "merged_spectral_contrast_df = merge_features(catalog_df_filtered, spectral_contrast_df, 'spectral')\n",
    "merged_vgg19_df = merge_features(catalog_df_filtered, vgg19_df, 'vgg19')\n",
    "merged_resnet_df = merge_features(catalog_df_filtered, resnet_df, 'resnet')\n",
    "\n",
    "#convert merged feature DataFrames\n",
    "bert_matrix = merged_bert_df.drop('id', axis=1).values\n",
    "mfcc_bow_matrix = merged_mfcc_bow_df.drop('id', axis=1).values\n",
    "spectral_contrast_matrix = merged_spectral_contrast_df.drop('id', axis=1).values\n",
    "vgg19_matrix = merged_vgg19_df.drop('id', axis=1).values\n",
    "resnet_matrix = merged_resnet_df.drop('id', axis=1).values\n",
    "\n",
    "#convert to sparse matrices if necessary\n",
    "bert_matrix = csr_matrix(bert_matrix)\n",
    "mfcc_bow_matrix = csr_matrix(mfcc_bow_matrix)\n",
    "spectral_contrast_matrix = csr_matrix(spectral_contrast_matrix)\n",
    "vgg19_matrix = csr_matrix(vgg19_matrix)\n",
    "resnet_matrix = csr_matrix(resnet_matrix)\n",
    "\n",
    "print(\"\\nFeature matrices have been merged and aligned with catalog_df_filtered.\")\n",
    "\n",
    "#feature matrices for retrieval functions that require them\n",
    "feature_matrices = {\n",
    "    'TF-IDF Retrieval': tag_matrix_tfidf,\n",
    "    'Tag-Based Retrieval': tag_matrix_binary,\n",
    "    'BERT Retrieval': bert_matrix,\n",
    "    'MFCC Retrieval': mfcc_bow_matrix,\n",
    "    'Spectral Contrast Retrieval': spectral_contrast_matrix,\n",
    "    'VGG19 Retrieval': vgg19_matrix,\n",
    "    'ResNet Retrieval': resnet_matrix\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#Retrieval Functions\n",
    "\n",
    "\n",
    "def random_retrieval(query_track_id, catalog_df, N=10):\n",
    "    \"\"\"\n",
    "    Randomly selects N tracks from the catalog, excluding the query track.\n",
    "    \n",
    "    Parameters:\n",
    "    - query_track_id (str): The ID of the query track.\n",
    "    - catalog_df (pd.DataFrame): The catalog containing all tracks.\n",
    "    - N (int): Number of tracks to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "    - retrieved_tracks (pd.DataFrame): DataFrame of retrieved tracks.\n",
    "    \"\"\"\n",
    "    if catalog_df is None:\n",
    "        raise ValueError(\"catalog_df must be provided for Random Retrieval.\")\n",
    "    \n",
    "    #exclude the query track\n",
    "    candidates = catalog_df[catalog_df['id'] != query_track_id]\n",
    "    \n",
    "    #determine the number of tracks to sample\n",
    "    sample_size = min(N, len(candidates))\n",
    "    \n",
    "    #randomly sample N tracks\n",
    "    retrieved_tracks = candidates.sample(n=sample_size, replace=False, random_state=random.randint(0, 1000000))\n",
    "    \n",
    "    return retrieved_tracks.reset_index(drop=True)\n",
    "\n",
    "def tfidf_retrieval(query_track_id, id_to_index, feature_matrix, track_ids, catalog_df, N=10):\n",
    "    \"\"\"\n",
    "    Retrieves N tracks most similar to the query track based on TF-IDF cosine similarity.\n",
    "    \"\"\"\n",
    "    if query_track_id not in id_to_index:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    query_index = id_to_index[query_track_id]\n",
    "    query_vector = feature_matrix[query_index]\n",
    "\n",
    "    similarities = cosine_similarity(query_vector, feature_matrix).flatten()\n",
    "\n",
    "    similarities[query_index] = -1\n",
    "\n",
    "    top_indices = similarities.argsort()[-N:][::-1]\n",
    "\n",
    "    retrieved_ids = [track_ids[i] for i in top_indices]\n",
    "    retrieved_scores = [similarities[i] for i in top_indices]\n",
    "\n",
    "    retrieved_tracks_df = pd.DataFrame({\n",
    "        'id': retrieved_ids,\n",
    "        'similarity': retrieved_scores\n",
    "    })\n",
    "\n",
    "    retrieved_tracks_df = pd.merge(retrieved_tracks_df, catalog_df[['id', 'artist', 'song', 'album_name']], on='id', how='left')\n",
    "\n",
    "    return retrieved_tracks_df\n",
    "\n",
    "def bert_retrieval(query_track_id, id_to_index, feature_matrix, track_ids, catalog_df, N=10):\n",
    "    \"\"\"\n",
    "    Retrieves N tracks most similar to the query track based on BERT cosine similarity.\n",
    "    \"\"\"\n",
    "    if query_track_id not in id_to_index:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    query_index = id_to_index[query_track_id]\n",
    "    query_vector = feature_matrix[query_index]\n",
    "    \n",
    "    similarities = cosine_similarity(query_vector, feature_matrix).flatten()\n",
    "    \n",
    "    similarities[query_index] = -1\n",
    "    \n",
    "    top_indices = similarities.argsort()[-N:][::-1]\n",
    "    \n",
    "    retrieved_ids = [track_ids[i] for i in top_indices]\n",
    "    retrieved_scores = [similarities[i] for i in top_indices]\n",
    "    \n",
    "    retrieved_tracks_df = pd.DataFrame({\n",
    "        'id': retrieved_ids,\n",
    "        'similarity': retrieved_scores\n",
    "    })\n",
    "    \n",
    "    retrieved_tracks_df = pd.merge(retrieved_tracks_df, catalog_df[['id', 'artist', 'song', 'album_name']], on='id', how='left')\n",
    "    \n",
    "    return retrieved_tracks_df\n",
    "\n",
    "def mfcc_retrieval(query_track_id, id_to_index, feature_matrix, track_ids, catalog_df, N=10):\n",
    "    \"\"\"\n",
    "    Retrieves N tracks most similar to the query track based on MFCC Euclidean distance.\n",
    "    \"\"\"\n",
    "    if query_track_id not in id_to_index:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    query_index = id_to_index[query_track_id]\n",
    "    query_vector = feature_matrix[query_index].toarray()\n",
    "    \n",
    "    distances = np.linalg.norm(feature_matrix - query_vector, axis=1)\n",
    "    \n",
    "    distances[query_index] = np.inf\n",
    "    \n",
    "    top_indices = distances.argsort()[:N]\n",
    "    \n",
    "    retrieved_ids = [track_ids[i] for i in top_indices]\n",
    "    retrieved_distances = distances[top_indices]\n",
    "    \n",
    "    retrieved_tracks_df = pd.DataFrame({\n",
    "        'id': retrieved_ids,\n",
    "        'distance': retrieved_distances\n",
    "    })\n",
    "    \n",
    "    retrieved_tracks_df = pd.merge(retrieved_tracks_df, catalog_df[['id', 'artist', 'song', 'album_name']], on='id', how='left')\n",
    "    \n",
    "    return retrieved_tracks_df\n",
    "\n",
    "def spectral_contrast_retrieval(query_track_id, id_to_index, feature_matrix, track_ids, catalog_df, N=10):\n",
    "    \"\"\"\n",
    "    Retrieves N tracks most similar to the query track based on Spectral Contrast Cosine similarity.\n",
    "    \"\"\"\n",
    "    if query_track_id not in id_to_index:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    query_index = id_to_index[query_track_id]\n",
    "    query_vector = feature_matrix[query_index]\n",
    "    \n",
    "    similarities = cosine_similarity(query_vector, feature_matrix).flatten()\n",
    "    \n",
    "    similarities[query_index] = -1\n",
    "    \n",
    "    top_indices = similarities.argsort()[-N:][::-1]\n",
    "    \n",
    "    retrieved_ids = [track_ids[i] for i in top_indices]\n",
    "    retrieved_scores = [similarities[i] for i in top_indices]\n",
    "    \n",
    "    retrieved_tracks_df = pd.DataFrame({\n",
    "        'id': retrieved_ids,\n",
    "        'similarity': retrieved_scores\n",
    "    })\n",
    "    \n",
    "    retrieved_tracks_df = pd.merge(retrieved_tracks_df, catalog_df[['id', 'artist', 'song', 'album_name']], on='id', how='left')\n",
    "    \n",
    "    return retrieved_tracks_df\n",
    "\n",
    "def vgg19_retrieval(query_track_id, id_to_index, feature_matrix, track_ids, catalog_df, N=10):\n",
    "    \"\"\"\n",
    "    Retrieves N tracks most similar to the query track based on VGG19 Cosine similarity.\n",
    "    \"\"\"\n",
    "    if query_track_id not in id_to_index:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    query_index = id_to_index[query_track_id]\n",
    "    query_vector = feature_matrix[query_index]\n",
    "    \n",
    "    similarities = cosine_similarity(query_vector, feature_matrix).flatten()\n",
    "    \n",
    "    similarities[query_index] = -1\n",
    "    \n",
    "    top_indices = similarities.argsort()[-N:][::-1]\n",
    "    \n",
    "    retrieved_ids = [track_ids[i] for i in top_indices]\n",
    "    retrieved_scores = [similarities[i] for i in top_indices]\n",
    "    \n",
    "    retrieved_tracks_df = pd.DataFrame({\n",
    "        'id': retrieved_ids,\n",
    "        'similarity': retrieved_scores\n",
    "    })\n",
    "    \n",
    "    retrieved_tracks_df = pd.merge(retrieved_tracks_df, catalog_df[['id', 'artist', 'song', 'album_name']], on='id', how='left')\n",
    "    \n",
    "    return retrieved_tracks_df\n",
    "\n",
    "def resnet_retrieval(query_track_id, id_to_index, feature_matrix, track_ids, catalog_df, N=10):\n",
    "    \"\"\"\n",
    "    Retrieves N tracks most similar to the query track based on ResNet Euclidean distance.\n",
    "    \"\"\"\n",
    "    if query_track_id not in id_to_index:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    query_index = id_to_index[query_track_id]\n",
    "    query_vector = feature_matrix[query_index].toarray()\n",
    "    \n",
    "    distances = np.linalg.norm(feature_matrix - query_vector, axis=1)\n",
    "    \n",
    "    distances[query_index] = np.inf\n",
    "    \n",
    "    top_indices = distances.argsort()[:N]\n",
    "    \n",
    "    retrieved_ids = [track_ids[i] for i in top_indices]\n",
    "    retrieved_distances = distances[top_indices]\n",
    "    \n",
    "    retrieved_tracks_df = pd.DataFrame({\n",
    "        'id': retrieved_ids,\n",
    "        'distance': retrieved_distances\n",
    "    })\n",
    "    \n",
    "    retrieved_tracks_df = pd.merge(retrieved_tracks_df, catalog_df[['id', 'artist', 'song', 'album_name']], on='id', how='left')\n",
    "    \n",
    "    return retrieved_tracks_df\n",
    "\n",
    "def tag_based_retrieval_cosine(query_track_id, id_to_index, feature_matrix, track_ids, catalog_df, N=10):\n",
    "    \"\"\"\n",
    "    Retrieves N tracks most similar to the query track based on Cosine similarity of binary tags.\n",
    "    \n",
    "    Parameters:\n",
    "    - query_track_id (str): The ID of the query track.\n",
    "    - id_to_index (dict): Mapping from track ID to index in feature_matrix.\n",
    "    - feature_matrix (csr_matrix): Binary tag feature matrix.\n",
    "    - track_ids (list): List of all track IDs.\n",
    "    - catalog_df (pd.DataFrame): DataFrame containing track metadata.\n",
    "    - N (int): Number of tracks to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "    - retrieved_tracks_df (pd.DataFrame): DataFrame of retrieved tracks with similarity scores.\n",
    "    \"\"\"\n",
    "    if query_track_id not in id_to_index:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    query_index = id_to_index[query_track_id]\n",
    "    query_vector = feature_matrix[query_index]\n",
    "    \n",
    "    #query_vector is non-zero\n",
    "    if query_vector.nnz == 0:\n",
    "        print(f\"Query track ID {query_track_id} has no significant tags. Skipping retrieval.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    similarities = cosine_similarity(query_vector, feature_matrix).flatten()\n",
    "    \n",
    "    similarities[query_index] = -1\n",
    "    \n",
    "    #top N similar tracks\n",
    "    top_indices = similarities.argsort()[-N:][::-1]\n",
    "    \n",
    "    \n",
    "    #retrieve track IDs and their similarity scores\n",
    "    retrieved_ids = [track_ids[i] for i in top_indices]\n",
    "    retrieved_scores = [similarities[i] for i in top_indices]\n",
    "    \n",
    "    #DataFrame for the retrieved tracks\n",
    "    retrieved_tracks_df = pd.DataFrame({\n",
    "        'id': retrieved_ids,\n",
    "        'cosine_similarity': retrieved_scores\n",
    "    })\n",
    "    \n",
    "    #include additional metadata\n",
    "    retrieved_tracks_df = pd.merge(retrieved_tracks_df, catalog_df[['id', 'artist', 'song', 'album_name']], on='id', how='left')\n",
    "    \n",
    "    return retrieved_tracks_df\n",
    "\n",
    "def early_fusion_retrieval(query_track_id, id_to_index, feature_matrix, track_ids, catalog_df, N=10):\n",
    "    \"\"\"\n",
    "    Retrieves N tracks using Early Fusion by combining BERT and MFCC feature matrices.\n",
    "    \"\"\"\n",
    "    if query_track_id not in id_to_index:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    query_index = id_to_index[query_track_id]\n",
    "    query_vector = feature_matrix[query_index]\n",
    "    \n",
    "    similarities = cosine_similarity(query_vector, feature_matrix).flatten()\n",
    "    \n",
    "    similarities[query_index] = -1\n",
    "    \n",
    "    top_indices = similarities.argsort()[-N:][::-1]\n",
    " \n",
    "    retrieved_ids = [track_ids[i] for i in top_indices]\n",
    "    retrieved_scores = [similarities[i] for i in top_indices]\n",
    "    \n",
    "    retrieved_tracks_df = pd.DataFrame({\n",
    "        'id': retrieved_ids,\n",
    "        'aggregated_similarity': retrieved_scores\n",
    "    })\n",
    "    \n",
    "    retrieved_tracks_df = pd.merge(retrieved_tracks_df, catalog_df[['id', 'artist', 'song', 'album_name']], on='id', how='left')\n",
    "    \n",
    "    return retrieved_tracks_df\n",
    "\n",
    "def late_fusion_retrieval(query_track_id, id_to_index, feature_matrices, track_ids, catalog_df, N=10, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Retrieves N tracks using Late Fusion by combining similarities from MFCC and VGG19 retrieval systems.\n",
    "    \n",
    "    Parameters:\n",
    "    - query_track_id (str): The ID of the query track.\n",
    "    - id_to_index (dict): Mapping from track ID to index in feature_matrix1 and feature_matrix2.\n",
    "    - feature_matrices (dict): Dictionary containing 'MFCC Retrieval' and 'VGG19 Retrieval' feature matrices.\n",
    "    - track_ids (list): List of all track IDs.\n",
    "    - catalog_df (pd.DataFrame): DataFrame containing track metadata.\n",
    "    - N (int): Number of tracks to retrieve.\n",
    "    - alpha (float): Weight parameter for the first similarity score; beta is implicitly (1 - alpha).\n",
    "    \n",
    "    Returns:\n",
    "    - retrieved_tracks_df (pd.DataFrame): DataFrame of retrieved tracks with aggregated similarity scores.\n",
    "    \"\"\"\n",
    "    if query_track_id not in id_to_index:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    query_index = id_to_index[query_track_id]\n",
    "    \n",
    "    feature_matrix1 = feature_matrices.get('MFCC Retrieval')\n",
    "    if feature_matrix1 is None:\n",
    "        print(\"Error: 'MFCC Retrieval' feature matrix not found.\")\n",
    "        return pd.DataFrame()\n",
    "    query_vector1 = feature_matrix1[query_index]\n",
    "    similarities1 = cosine_similarity(query_vector1, feature_matrix1).flatten()\n",
    "    similarities1[query_index] = -1 \n",
    "    \n",
    "    feature_matrix2 = feature_matrices.get('VGG19 Retrieval')\n",
    "    if feature_matrix2 is None:\n",
    "        print(\"Error: 'VGG19 Retrieval' feature matrix not found.\")\n",
    "        return pd.DataFrame()\n",
    "    query_vector2 = feature_matrix2[query_index]\n",
    "    similarities2 = cosine_similarity(query_vector2, feature_matrix2).flatten()\n",
    "    similarities2[query_index] = -1  # Exclude query\n",
    "    \n",
    "    #weighted average of similarities\n",
    "    aggregated_similarities = alpha * similarities1 + (1 - alpha) * similarities2\n",
    "    \n",
    "    top_indices = aggregated_similarities.argsort()[-N:][::-1]\n",
    "    \n",
    "    retrieved_ids = [track_ids[i] for i in top_indices]\n",
    "    retrieved_scores = [aggregated_similarities[i] for i in top_indices]\n",
    "    \n",
    "    retrieved_tracks_df = pd.DataFrame({\n",
    "        'id': retrieved_ids,\n",
    "        'aggregated_similarity': retrieved_scores\n",
    "    })\n",
    "    \n",
    "    retrieved_tracks_df = pd.merge(retrieved_tracks_df, catalog_df[['id', 'artist', 'song', 'album_name']], on='id', how='left')\n",
    "    \n",
    "    return retrieved_tracks_df\n",
    "\n",
    "\n",
    "\n",
    "#Retrieval Systems\n",
    "retrieval_systems = {\n",
    "    'Random Retrieval': random_retrieval,\n",
    "    'Tag-Based Retrieval': tag_based_retrieval_cosine,  # Using Cosine Similarity\n",
    "    'TF-IDF Retrieval': tfidf_retrieval,\n",
    "    'BERT Retrieval': bert_retrieval,\n",
    "    'MFCC Retrieval': mfcc_retrieval,\n",
    "    'Spectral Contrast Retrieval': spectral_contrast_retrieval,\n",
    "    'VGG19 Retrieval': vgg19_retrieval,\n",
    "    'ResNet Retrieval': resnet_retrieval,\n",
    "    'Early Fusion BERT+MFCC Retrieval': early_fusion_retrieval,\n",
    "    'Late Fusion MFCC+VGG19 Retrieval': late_fusion_retrieval\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#Track IDs and Index Mapping\n",
    "\n",
    "track_ids = catalog_df_filtered['id'].tolist()\n",
    "id_to_index = {track_id: idx for idx, track_id in enumerate(track_ids)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Evaluation Metrics\n",
    "\n",
    "def precision_at_k(retrieved_ids, relevant_ids, k=10):\n",
    "    \"\"\"\n",
    "    Computes Precision@k.\n",
    "    \"\"\"\n",
    "    retrieved_set = set(retrieved_ids[:k])\n",
    "    relevant_set = set(relevant_ids)\n",
    "    precision = len(retrieved_set & relevant_set) / k\n",
    "    return precision\n",
    "\n",
    "def recall_at_k(retrieved_ids, relevant_ids, k=10):\n",
    "    \"\"\"\n",
    "    Computes Recall@k.\n",
    "    \"\"\"\n",
    "    retrieved_set = set(retrieved_ids[:k])\n",
    "    relevant_set = set(relevant_ids)\n",
    "    recall = len(retrieved_set & relevant_set) / len(relevant_set) if relevant_set else 0\n",
    "    return recall\n",
    "\n",
    "def ndcg_at_k(retrieved_ids, relevant_ids, k=10):\n",
    "    \"\"\"\n",
    "    Computes NDCG@k.\n",
    "    \"\"\"\n",
    "    dcg = 0.0\n",
    "    for i, track_id in enumerate(retrieved_ids[:k]):\n",
    "        if track_id in relevant_ids:\n",
    "            dcg += 1 / np.log2(i + 2)\n",
    "\n",
    "    # Ideal DCG\n",
    "    ideal_relevant = min(len(relevant_ids), k)\n",
    "    ideal_dcg = sum([1 / np.log2(i + 2) for i in range(ideal_relevant)])\n",
    "\n",
    "    return dcg / ideal_dcg if ideal_dcg > 0 else 0.0\n",
    "\n",
    "def mrr_metric(retrieved_ids, relevant_ids):\n",
    "    \"\"\"\n",
    "    Computes Mean Reciprocal Rank (MRR).\n",
    "    \"\"\"\n",
    "    for rank, track_id in enumerate(retrieved_ids, start=1):\n",
    "        if track_id in relevant_ids:\n",
    "            return 1.0 / rank\n",
    "    return 0.0\n",
    "\n",
    "def compute_cov_at_n(all_retrieved_ids, catalog_df, N=10):\n",
    "    \"\"\"\n",
    "    Computes Coverage@N: Percentage of songs that appear in at least one retrieval list.\n",
    "    \"\"\"\n",
    "    flattened = [track_id for sublist in all_retrieved_ids for track_id in sublist[:N]]\n",
    "    unique_retrieved = set(flattened)\n",
    "    total_tracks = len(catalog_df)\n",
    "    coverage = (len(unique_retrieved) / total_tracks) * 100\n",
    "    return coverage\n",
    "\n",
    "def compute_div_at_n(all_retrieved_tags, N=10):\n",
    "    \"\"\"\n",
    "    Computes Diversity@N: Average number of unique tag occurrences among retrieved songs.\n",
    "    \"\"\"\n",
    "    diversity_scores = []\n",
    "    for tags in all_retrieved_tags:\n",
    "        top_n_tags = tags[:N]\n",
    "        unique_tags = set(top_n_tags)\n",
    "        diversity_scores.append(len(unique_tags))\n",
    "    average_diversity = np.mean(diversity_scores) if diversity_scores else 0\n",
    "    return average_diversity\n",
    "\n",
    "def compute_avg_pop_at_n(all_retrieved_popularity, N=10):\n",
    "    \"\"\"\n",
    "    Computes AvgPop@N: Average popularity of retrieved songs.\n",
    "    \"\"\"\n",
    "    avg_popularity_scores = []\n",
    "    for pops in all_retrieved_popularity:\n",
    "        top_n_pops = pops[:N]\n",
    "        if top_n_pops:\n",
    "            avg_popularity_scores.append(np.mean(top_n_pops))\n",
    "    average_popularity = np.mean(avg_popularity_scores) if avg_popularity_scores else 0\n",
    "    return average_popularity\n",
    "\n",
    "def compute_popularity_diversity(all_retrieved_popularity, N=10):\n",
    "    \"\"\"\n",
    "    Computes Popularity Diversity@N: Variance of popularity scores among retrieved songs.\n",
    "    \"\"\"\n",
    "    diversity_scores = []\n",
    "    for pops in all_retrieved_popularity:\n",
    "        top_n_pops = pops[:N]\n",
    "        if len(top_n_pops) > 1:\n",
    "            diversity_scores.append(np.var(top_n_pops))\n",
    "        else:\n",
    "            diversity_scores.append(0)\n",
    "    average_diversity = np.mean(diversity_scores) if diversity_scores else 0\n",
    "    return average_diversity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Evaluation Functions\n",
    "\n",
    "def evaluate_retrieval_system(\n",
    "    catalog_df,\n",
    "    track_ids,\n",
    "    id_to_index,\n",
    "    retrieval_function,\n",
    "    feature_matrix=None,\n",
    "    N=10,\n",
    "    retrieval_system_name='',\n",
    "    feature_matrices=None,\n",
    "    alpha=0.5,\n",
    "    relevance_definition='top_genre'\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates a retrieval system, computing both accuracy and beyond-accuracy metrics based on the relevance definition.\n",
    "\n",
    "    Parameters:\n",
    "    - catalog_df (pd.DataFrame): DataFrame containing all tracks with 'id', 'tags', 'popularity', 'top_genre', and 'genre'.\n",
    "    - track_ids (list): List of track IDs.\n",
    "    - id_to_index (dict): Mapping from track ID to index.\n",
    "    - retrieval_function (function): The specific retrieval function for the IR system.\n",
    "    - feature_matrix (csr_matrix, optional): Feature matrix used by the retrieval function.\n",
    "    - N (int): Number of tracks to retrieve per query.\n",
    "    - retrieval_system_name (str): Name of the retrieval system (for logging).\n",
    "    - feature_matrices (dict, optional): Additional feature matrices for late fusion.\n",
    "    - alpha (float): Weight parameter for late fusion.\n",
    "    - relevance_definition (str): 'top_genre' or 'tag_overlap'\n",
    "\n",
    "    Returns:\n",
    "    - metrics (dict): Dictionary containing all evaluation metrics.\n",
    "    \"\"\"\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    ndcgs = []\n",
    "    mrrs = []\n",
    "\n",
    "    all_retrieved_ids = []\n",
    "    all_retrieved_tags = []\n",
    "    all_retrieved_popularity = []\n",
    "\n",
    "    total_queries = len(catalog_df)\n",
    "    processed_queries = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for index, query_track in catalog_df.iterrows():\n",
    "        query_id = query_track['id']\n",
    "        \n",
    "        #relevance based on the chosen relevance_definition\n",
    "        if relevance_definition == 'top_genre':\n",
    "            query_genre = query_track['top_genre']\n",
    "            if not query_genre:\n",
    "                continue  # Skip if no genre\n",
    "            relevant_ids = catalog_df[catalog_df['top_genre'] == query_genre]['id'].tolist()\n",
    "        elif relevance_definition == 'tag_overlap':\n",
    "            query_tags = set(query_track['filtered_processed_tags_final'])\n",
    "            relevant_ids = catalog_df[catalog_df['filtered_processed_tags_final'].apply(lambda tags: len(query_tags.intersection(tags)) >= 3)]['id'].tolist()\n",
    "        else:\n",
    "            print(f\"Unknown relevance_definition: {relevance_definition}\")\n",
    "            return {}\n",
    "        \n",
    "        #retrieval\n",
    "        if retrieval_system_name == 'Late Fusion MFCC+VGG19 Retrieval' and feature_matrices:\n",
    "            retrieved_tracks = retrieval_function(\n",
    "                query_track_id=query_id,\n",
    "                id_to_index=id_to_index,\n",
    "                feature_matrices=feature_matrices,\n",
    "                track_ids=track_ids,\n",
    "                catalog_df=catalog_df,\n",
    "                N=N,\n",
    "                alpha=alpha\n",
    "            )\n",
    "        elif feature_matrix is not None:\n",
    "            retrieved_tracks = retrieval_function(\n",
    "                query_track_id=query_id,\n",
    "                id_to_index=id_to_index,\n",
    "                feature_matrix=feature_matrix,\n",
    "                track_ids=track_ids,\n",
    "                catalog_df=catalog_df,\n",
    "                N=N\n",
    "            )\n",
    "        else:\n",
    "            retrieved_tracks = retrieval_function(\n",
    "                query_track_id=query_id,\n",
    "                catalog_df=catalog_df,\n",
    "                N=N\n",
    "            )\n",
    "\n",
    "        if retrieved_tracks.empty:\n",
    "            continue\n",
    "\n",
    "        retrieved_ids = retrieved_tracks['id'].tolist()\n",
    "\n",
    "        #tags and popularity for retrieved tracks\n",
    "        retrieved_subset = catalog_df[catalog_df['id'].isin(retrieved_ids)]\n",
    "        retrieved_tags = retrieved_subset['filtered_processed_tags_final'].tolist()\n",
    "        retrieved_popularity = retrieved_subset['popularity'].tolist()\n",
    "\n",
    "        #evaluation metrics\n",
    "        p = precision_at_k(retrieved_ids, relevant_ids, k=N)\n",
    "        r = recall_at_k(retrieved_ids, relevant_ids, k=N)\n",
    "        ndcg = ndcg_at_k(retrieved_ids, relevant_ids, k=N)\n",
    "        rr = mrr_metric(retrieved_ids, relevant_ids)\n",
    "\n",
    "        precisions.append(p)\n",
    "        recalls.append(r)\n",
    "        ndcgs.append(ndcg)\n",
    "        mrrs.append(rr)\n",
    "\n",
    "        #data for beyond-accuracy metrics\n",
    "        all_retrieved_ids.append(retrieved_ids)\n",
    "        #flatten the list of tags for each retrieved song\n",
    "        flattened_tags = [tag for sublist in retrieved_tags for tag in sublist]\n",
    "        all_retrieved_tags.append(flattened_tags)\n",
    "        all_retrieved_popularity.append(retrieved_popularity)\n",
    "\n",
    "        processed_queries += 1\n",
    "        if processed_queries % 500 == 0:\n",
    "            print(f\"Processed {processed_queries}/{total_queries} queries\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Evaluation completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    #aggregated accuracy metrics\n",
    "    accuracy_metrics = {\n",
    "        'Precision@10': np.mean(precisions) if precisions else 0,\n",
    "        'Recall@10': np.mean(recalls) if recalls else 0,\n",
    "        'NDCG@10': np.mean(ndcgs) if ndcgs else 0,\n",
    "        'MRR': np.mean(mrrs) if mrrs else 0\n",
    "    }\n",
    "\n",
    "    #beyond-accuracy metrics\n",
    "    coverage = compute_cov_at_n(all_retrieved_ids, catalog_df, N)\n",
    "    tag_diversity = compute_div_at_n(all_retrieved_tags, N)\n",
    "    popularity_diversity = compute_popularity_diversity(all_retrieved_popularity, N)\n",
    "    avg_popularity = compute_avg_pop_at_n(all_retrieved_popularity, N)\n",
    "\n",
    "    metrics = {\n",
    "        **accuracy_metrics,\n",
    "        'Coverage@10': coverage,\n",
    "        'Tag Diversity@10': tag_diversity,\n",
    "        'Popularity Diversity@10': popularity_diversity,\n",
    "        'AvgPop@10': avg_popularity\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "#AvgNumber of Tags and Genres for Baseline Comparison\n",
    "\n",
    "avg_tags_per_track = catalog_df_filtered['filtered_processed_tags_final'].apply(len).mean()\n",
    "avg_genres_per_track = catalog_df_filtered['genre'].apply(len).mean()\n",
    "print(f\"\\nAverage number of significant tags per track: {avg_tags_per_track:.2f}\")\n",
    "print(f\"Average number of genres per track: {avg_genres_per_track:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#20. Evaluate\n",
    "\n",
    "#store results\n",
    "results_list = []\n",
    "\n",
    "#Relevance Definitions\n",
    "relevance_definitions = ['top_genre', 'tag_overlap']\n",
    "\n",
    "print(\"\\nStarting Evaluation of Retrieval Systems...\\n\")\n",
    "\n",
    "for relevance in relevance_definitions:\n",
    "    print(f\"--- Evaluating Relevance Definition: {relevance} ---\\n\")\n",
    "    for name, func in retrieval_systems.items():\n",
    "        print(f\"Evaluating {name} under relevance '{relevance}'...\")\n",
    "        if name == 'Late Fusion MFCC+VGG19 Retrieval':\n",
    "            # Late Fusion uses specific feature matrices\n",
    "            metrics = evaluate_retrieval_system(\n",
    "                catalog_df=catalog_df_filtered,\n",
    "                track_ids=track_ids,\n",
    "                id_to_index=id_to_index,\n",
    "                retrieval_function=func,\n",
    "                feature_matrix=None,  # Feature matrices are handled inside the retrieval function\n",
    "                N=10,\n",
    "                retrieval_system_name=name,\n",
    "                feature_matrices={\n",
    "                    'MFCC Retrieval': feature_matrices['MFCC Retrieval'],\n",
    "                    'VGG19 Retrieval': feature_matrices['VGG19 Retrieval']\n",
    "                },\n",
    "                alpha=0.5,\n",
    "                relevance_definition=relevance\n",
    "            )\n",
    "        elif name == 'Early Fusion BERT+MFCC Retrieval':\n",
    "            # Early Fusion uses the combined feature matrix\n",
    "            #combine BERT and MFCC features\n",
    "            if 'BERT Retrieval' in feature_matrices and 'MFCC Retrieval' in feature_matrices:\n",
    "                combined_feature_matrix = hstack([feature_matrices['BERT Retrieval'], feature_matrices['MFCC Retrieval']]).tocsr()\n",
    "                # Verify alignment\n",
    "                if combined_feature_matrix.shape[0] != len(track_ids):\n",
    "                    print(f\"Error: Combined feature matrix for '{name}' is not aligned with track_ids.\")\n",
    "                    continue\n",
    "                metrics = evaluate_retrieval_system(\n",
    "                    catalog_df=catalog_df_filtered,\n",
    "                    track_ids=track_ids,\n",
    "                    id_to_index=id_to_index,\n",
    "                    retrieval_function=func,\n",
    "                    feature_matrix=combined_feature_matrix,\n",
    "                    N=10,\n",
    "                    retrieval_system_name=name,\n",
    "                    relevance_definition=relevance\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Error: Required feature matrices for '{name}' are missing.\")\n",
    "                continue\n",
    "        elif name == 'Random Retrieval':\n",
    "            #RR does not require a feature matrix\n",
    "            metrics = evaluate_retrieval_system(\n",
    "                catalog_df=catalog_df_filtered,\n",
    "                track_ids=track_ids,\n",
    "                id_to_index=id_to_index,\n",
    "                retrieval_function=func,\n",
    "                feature_matrix=None,\n",
    "                N=10,\n",
    "                retrieval_system_name=name,\n",
    "                relevance_definition=relevance\n",
    "            )\n",
    "        else:\n",
    "            #retrieval systems that require a feature matrix\n",
    "            feature_matrix = feature_matrices.get(name)\n",
    "            if feature_matrix is None:\n",
    "                print(f\"Error: Feature matrix for '{name}' not found or misaligned.\")\n",
    "                continue\n",
    "            #alignment between feature_matrix and track_ids\n",
    "            if feature_matrix.shape[0] != len(track_ids):\n",
    "                print(f\"Error: Feature matrix rows ({feature_matrix.shape[0]}) do not match number of track_ids ({len(track_ids)}).\")\n",
    "                continue\n",
    "            metrics = evaluate_retrieval_system(\n",
    "                catalog_df=catalog_df_filtered,\n",
    "                track_ids=track_ids,\n",
    "                id_to_index=id_to_index,\n",
    "                retrieval_function=func,\n",
    "                feature_matrix=feature_matrix,\n",
    "                N=10,\n",
    "                retrieval_system_name=name,\n",
    "                relevance_definition=relevance\n",
    "            )\n",
    "        #results with system name and relevance definition\n",
    "        metrics_record = metrics.copy()\n",
    "        metrics_record['Retrieval System'] = name\n",
    "        metrics_record['Relevance Definition'] = relevance\n",
    "        results_list.append(metrics_record)\n",
    "        print(f\"Completed evaluation for {name} under relevance '{relevance}'.\\n\")\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "cols = ['Retrieval System', 'Relevance Definition', 'Precision@10', 'Recall@10', 'NDCG@10', 'MRR',\n",
    "        'Coverage@10', 'Tag Diversity@10', 'Popularity Diversity@10', 'AvgPop@10']\n",
    "results_df = results_df[cols]\n",
    "\n",
    "\n",
    "#Save results\n",
    "\n",
    "results_dir = 'evaluation_results'\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_filename = f\"evaluation_results_{timestamp}.csv\"\n",
    "results_filepath = os.path.join(results_dir, results_filename)\n",
    "\n",
    "#save results to a CSV file\n",
    "results_df.to_csv(results_filepath, index=False)\n",
    "print(f\"\\nEvaluation results saved to '{results_filepath}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "968327cd-1b33-44c4-8117-1210439b9053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded evaluation results from 'evaluation_results\\evaluation_results_20250112_012151.csv'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the directory and file path for results\n",
    "results_dir = 'evaluation_results'\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "# Define the CSV filename\n",
    "results_filename = \"evaluation_results_20250112_012151.csv\"\n",
    "results_filepath = os.path.join(results_dir, results_filename)\n",
    "\n",
    "# Configure Seaborn aesthetics\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\")  # Smaller font size for reduced image size\n",
    "\n",
    "def visualize_top_genre_results(csv_filepath):\n",
    "    \"\"\"\n",
    "    Reads evaluation results from a CSV file and generates visualizations for 'top_genre' only.\n",
    "    \n",
    "    Parameters:\n",
    "    - csv_filepath (str): Path to the CSV file containing evaluation results.\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    eval_df = pd.read_csv(csv_filepath)\n",
    "    print(f\"\\nLoaded evaluation results from '{csv_filepath}'.\\n\")\n",
    "    \n",
    "    # Filter data for 'top_genre' relevance definition\n",
    "    top_genre_df = eval_df[eval_df['Relevance Definition'] == 'top_genre']\n",
    "    \n",
    "    # Ensure the 'plots' directory exists\n",
    "    os.makedirs('plots', exist_ok=True)\n",
    "    \n",
    "    # Bar Plot: Metrics Comparison Across Retrieval Systems for 'top_genre'\n",
    "    metrics_to_plot = ['Precision@10', 'Recall@10', 'NDCG@10', 'MRR', \n",
    "                       'Coverage@10', 'Tag Diversity@10', \n",
    "                       'Popularity Diversity@10', 'AvgPop@10']\n",
    "    \n",
    "    for metric in metrics_to_plot:\n",
    "        plt.figure(figsize=(8, 4))  # Reduced size\n",
    "        sns.barplot(x='Retrieval System', y=metric, data=top_genre_df, palette='viridis')\n",
    "        plt.title(f'{metric} Across Retrieval Systems (top_genre)', fontsize=12)\n",
    "        plt.xlabel('Retrieval System', fontsize=10)\n",
    "        plt.ylabel(metric, fontsize=10)\n",
    "        plt.xticks(rotation=45, fontsize=8)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"plots/{metric.replace('@', '_').replace(' ', '_').lower()}_barplot_top_genre.png\", dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # Scatter Plot: Pairwise Metric Relationships for 'top_genre'\n",
    "    plot_configs = [\n",
    "        {\n",
    "            'x': 'Coverage@10',\n",
    "            'y': 'NDCG@10',\n",
    "            'title': 'NDCG@10 vs Coverage@10 (top_genre)',\n",
    "            'xlabel': 'Coverage@10 (%)',\n",
    "            'ylabel': 'NDCG@10',\n",
    "            'filename': 'ndcg_vs_coverage_top_genre.png'\n",
    "        },\n",
    "        {\n",
    "            'x': 'Tag Diversity@10',\n",
    "            'y': 'NDCG@10',\n",
    "            'title': 'NDCG@10 vs Tag Diversity@10 (top_genre)',\n",
    "            'xlabel': 'Tag Diversity@10',\n",
    "            'ylabel': 'NDCG@10',\n",
    "            'filename': 'ndcg_vs_tag_diversity_top_genre.png'\n",
    "        },\n",
    "        {\n",
    "            'x': 'Popularity Diversity@10',\n",
    "            'y': 'NDCG@10',\n",
    "            'title': 'NDCG@10 vs Popularity Diversity@10 (top_genre)',\n",
    "            'xlabel': 'Popularity Diversity@10',\n",
    "            'ylabel': 'NDCG@10',\n",
    "            'filename': 'ndcg_vs_popularity_diversity_top_genre.png'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for config in plot_configs:\n",
    "        plt.figure(figsize=(8, 4))  # Reduced size\n",
    "        sns.scatterplot(\n",
    "            data=top_genre_df,\n",
    "            x=config['x'],\n",
    "            y=config['y'],\n",
    "            hue='Retrieval System',\n",
    "            s=50,  # Smaller marker size\n",
    "            palette='deep'\n",
    "        )\n",
    "        plt.title(config['title'], fontsize=12)\n",
    "        plt.xlabel(config['xlabel'], fontsize=10)\n",
    "        plt.ylabel(config['ylabel'], fontsize=10)\n",
    "        plt.legend(title='IR System', fontsize=8, title_fontsize=10, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"plots/{config['filename']}\", dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "# Call the function to visualize the results for 'top_genre'\n",
    "visualize_top_genre_results(results_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60216aed-2b36-41b9-bc9d-11bab8d53b18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
